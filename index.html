<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Uni-OVSeg: Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision">
  <meta name="keywords" content="Open-Vocabulary Segmentation, Weakly-supervised learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Uni-OVSeg: Open-Vocabulary Segmentation with Unpaired Mask-Text
              Supervision</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://derrickwang005.github.io">Zhaoqing Wang</a><sup>1,4</sup> </span>&nbsp;&nbsp;
              <a href="https://xiaoboxia.github.io">Xiaobo Xia</a><sup>1</sup> </span>&nbsp;&nbsp;
              <a href="https://czyczyyzc.github.io/">Ziye Chen</a><sup>2</sup>&nbsp;&nbsp;
              Xiao He<sup>4</sup>&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ">Yandong Guo</a><sup>4</sup>&nbsp;&nbsp;
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
              <a href="https://mingming-gong.github.io/">Mingming Gong</a><sup>2,3,üòé</sup>&nbsp;&nbsp;
              <a href="https://tongliang-liu.github.io">Tongliang Liu</a><sup>1,üòé</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>The University of Sydney</span>&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>The University of Melbourne</span> &nbsp;&nbsp;
              <span class="author-block"><sup>3</sup>Mohamed bin Zayed University of Artificial
                Intelligence</span>&nbsp;&nbsp;
              <span class="author-block"><sup>4</sup>AI2Robotics</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">üòé Corresponding authors</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2402.08960.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/DerrickWang005/Uni-OVSeg.pytorch"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">Motivation</h2>
          <div class="content has-text-justified">
            <figure>
              <img src="./static/images/teaser.png" id="aa" />
              <figcaption>Figure 1. The proposed Uni-OVSeg framework learns open-vocabulary segmentation with unpaired mask-text supervision. Compared to
                the labour-intensive image-mask-text annotations, independent image-mask and image-text pairs are easier to collect. With one suite of
                weights, given different visual prompts (e.g., points and boxes), Uni-OVSeg can segment and categorise various objects and stuff from an
                open set of vocabulary in the real world.
                </figcaption>
            </figure>
            <p>Cutting-edge approaches in open-vocabulary segmentation typically leverage supervision with triplet annotations
              that are composed of images, masks, and corresponding texts [1, 2].
              It is worth noting that strict alignment between each mask and text results in an expensive annotation cost.
              To mitigate this, some weakly-supervised methods propose using only text supervision [3, 4].
            </p>
            <p>
              However, learning with this supervision, the model falls short in capturing complex spatial details,
              which is suboptimal for dense prediction.
              Furthermore, this type of supervision lacks positional information, making the model difficult to
              distinguish different instances with the same semantic class.
              These issues severely limit the versatility and segmentation performance of existing weakly-supervised methods.
            </p>
            <p>
              As shown in Figure 1., we propose an advanced weakly-supervised open-vocabulary segmentation framework,
              named Uni-OVSeg, to reduce the annotation expense while significantly enhancing performance.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <figure>
              <img src="./static/images/architecture.png" id="aa" />
              <figcaption>Figure 2. Overview of the Uni-OVSeg framework.
                This framework consists of feature extraction, mask generation, and mask-text alignment.
                A frozen CLIP model and prompt encoder are used for image and text feature extraction and prompt encoding, respectively.
                We employs a mask and pixel decoder for binary mask prediction.
                A mask-text bipartite matching is designed to exploit confident mask-entity pairs.
                Visual prompts using boxes are omitted for simplicity.</figcaption>
            </figure>
            <p>
              On a macro level, as illustrated in Figure 2., the proposed Uni-OVSeg contains a CLIP model to extract features of both images and text descriptions.
              With the image-mask pairs, a branch of mask generation, including a visual prompt encoder, a pixel decoder and a mask decoder,
              is employed to predict a set of binary masks of an input image. With the image-text pairs,
              mask-text bipartite matching is used to exploit confident pairs between predicted masks and entities in text descriptions.
              Afterward, we adopt a multi-scale feature adapter to enhance the mask-wise visual embeddings,
              which are further aligned with associated entity embeddings based on the confident pairs.
              Finally, we perform open-vocabulary segmentation with the above-mentioned parts.
              More details can be found in our paper.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">Results</h2>

          <!-- Interpolating. -->
          <div class="content has-text-justified">
            <h3 class="title is-4">1. Open-vocabulary semantic segmentation performance</h3>
            <figure>
              <figcaption>Table 1. Open-vocabulary semantic segmentation performance.
                We mainly compare with the fully-supervised and weakly-supervised methods.
                ‚ÄúCOCO S.‚Äù, ‚ÄúCOCO P.‚Äù and ‚ÄúCOCO C.‚Äù denote the COCO stuff, panoptic and caption datasets.
                ‚ÄúO365‚Äù denotes the Object 365 dataset. ‚ÄúM. 41M‚Äù denotes the merged 41M image dataset.
                We report mIoU for all datasets.
              </figcaption>
              <img src="./static/images/semseg.png" id="aa" />
            </figure>
            <p>
              As demonstrated in Table 1., we compare our method to previous works across a range of benchmarks,
              including ADE20K (encompassing both 150 and 847 class variants), PASCAL Context (459 and 59 class variants), PASCAL VOC (with 20 and 21 class categories), and Cityscapes.
              Compared to weakly-supervised methods, Uni-OVSeg exhibits remarkable performance improvements across all evaluated datasets.
              Specifically, in the more challenging datasets of PASCAL Context-459, Uni-OVSeg not only surpasses its weakly-supervised counterparts but also outperforms the cutting-edge fully-supervised methods,
              e.g., FC-CLIP. This is indicative of Uni-OVSeg's superior capability in categorizing a diverse array of semantic classes.
              Furthermore, in the PASCAL VOC benchmarks (20 and 21 classes), Uni-OVSeg demonstrates a substantial enhancement over state-of-the-art weakly-supervised methods,
              achieving improvements of 18.3% and 12.2% mIoU, respectively, which demonstrates our Uni-OVSeg captures fine-grained spatial structures.
              These results elevate the practical applicability of weakly-supervised open-vocabulary segmentation to new heights.
            </p>
          </div>

          <div class="content has-text-justified">
            <h3 class="title is-4">2. Open-vocabulary panoptic segmentation performance</h3>
            <figure>
              <figcaption>Table 2. Open-vocabulary panoptic segmentation performance.
                We mainly compare with the fully-supervised and unsupervised methods.
                ‚ÄúCOCO P.‚Äù denotes the COCO panoptic datasets. ‚ÄúCOCO‚Äù denotes the COCO image dataset. ‚ÄúIN 1K‚Äù denotes the ImageNet-1K image dataset.
                We report PQ, SQ and RQ for all datasets.
              </figcaption>
              <img src="./static/images/panoseg.png" id="aa" />
            </figure>
            <p>
              As demonstrated in Table 2., for open-vocabulary panoptic segmentation, we zero-shot evaluate our model on the COCO, ADE20K, and Cityscapes datasets.
              Existing weakly-supervised methods only use text supervision, which causes a challenge to discriminate different instances with the same semantic class.
              To the best of our knowledge, we are the first to learn open-vocabulary panoptic segmentation with weak supervision.
              Compared to unsupervised methods, we obviously outperform U2Seg by 1.9% PQ, 1.5%SQ, and 4.4% RQ on COCO datasets.
              Unfortunately, our used image-mask pairs contain multiple granularity masks, such as object-wise and part-wise masks,
              which is different with the panoptic segmentation datasets. This discrepancy leads to a number of false positive results, limiting our performance.
            </p>
          </div>

          <div class="content has-text-justified">
            <h3 class="title is-4">3. Promptable segmentation performance</h3>
            <figure>
              <img src="./static/images/point_seg1.png" id="aa" />
              <figcaption>Figure 3. Point-promptable segmentation performance. We compare our method with SAM-ViT/L on a wide range of datasets.
                Given a 20 √ó 20 point grid as visual prompt, we select the output masks with max IoU by calculating the IoU with the ground-truth masks.
                We report 1-pt IoU for all datasets.
              </figcaption>
            </figure>
            <figure>
              <img src="./static/images/box_seg1.png" id="aa" />
              <figcaption>Figure 4. Box-promptable segmentation performance. We compare our method with SAM-ViT/L on a wide range of datasets.
                Given a ground-truth box as the visual prompt, we select the output masks with max IoU by calculating the IoU with the ground-truth
                masks. We report 1-pt IoU for all datasets.
              </figcaption>
            </figure>
            <figure>
              <img src="./static/images/point_seg2.png" id="aa" />
              <figcaption>Figure 5. Point-promptable segmentation performance. We compare our method with SAM-ViT/L on the SegInW datasets.
                Given a 20 √ó 20 point grid as a visual prompt, we select the output masks with max IoU by calculating the IoU with the ground-truth
                masks. We report 1-pt IoU for all datasets.                
              </figcaption>
            </figure>
            <figure>
              <img src="./static/images/box_seg2.png" id="aa" />
              <figcaption>Figure 6. Box-promptable segmentation performance. We compare our method with SAM-ViT/L on the SegInW datasets.
                Given a ground-truth box as the visual prompt, we select the output masks with max IoU by calculating the IoU with the ground-truth
                masks. We report 1-pt IoU for all datasets.
              </figcaption>
            </figure>
            <p>
              To evaluate the segmentation quality of Uni-OVSeg in scenarios involving interactive point and box prompts,
              we conduct comparative analyses with the SAM-ViT/L model across a range of datasets from diverse domains.
              For visual prompts, we implement a uniform 20 √ó 20 point grid as the interactive point prompt and utilise the actual bounding boxes as box prompts.
              The segmentation performance is measured using the 1-pt IoU (Oracle) metric across all datasets.
              We report results in Figures 3-6.
            </p>
          </div>

        </div>
      </div>
      <br />
      <!--/ Interpolating. -->
      <!--/ Animation. -->


      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">Related works</h2>

          <div class="content has-text-justified">
            <p>
              1.<a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.pdf"> Cris: Clip-driven referring image segmentation</a>.
            </p>
            <p>
              2.<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/661caac7729aa7d8c6b8ac0d39ccbc6a-Paper-Conference.pdf"> Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip</a>.
            </p>
            <p>
              3.<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.pdf"> Groupvit: Semantic segmentation emerges from text supervision</a>.
            </p>
            <p>
              4.<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Open-Vocabulary_Semantic_Segmentation_Models_From_Natural_Language_Supervision_CVPR_2023_paper.pdf"> Learning open-vocabulary semantic segmentation models from natural language supervision</a>.
            </p>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{wang2024open,
  title={Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision},
  author={Wang, Zhaoqing and Xia, Xiaobo and Chen, Ziye and He, Xiao and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  journal={arXiv preprint arXiv:2402.08960},
  year={2024}
}
      </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is forked from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies
                website</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>